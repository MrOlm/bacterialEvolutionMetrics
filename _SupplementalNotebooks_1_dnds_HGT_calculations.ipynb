{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import scipy\n",
    "import pickle\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "pd.set_option('display.max_rows', 100)\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "pd.set_option('display.max_columns', 100)\n",
    "\n",
    "sys.path.append('/home/mattolm/Bio_scripts/')\n",
    "import StatsTools\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load FastANI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FAdb = pd.read_pickle('/data1/bio_db/refseq/analysis/fastANI/mutlipleComps_v6.pickle')\n",
    "FAdb = FAdb.set_index(['genome1', 'genome2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_ani(FAdb, g1, g2):\n",
    "    try:\n",
    "        db = FAdb.loc[g1, g2]\n",
    "    except:\n",
    "        db = FAdb.loc[g2, g1]\n",
    "    \n",
    "    return db['fast_ani']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MAG data with HGT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def summarize_dnds_HGT(db, noFilt=False, fastANI=False):\n",
    "    table = defaultdict(list)\n",
    "\n",
    "    # Basic info\n",
    "    table['reference'].append(db['reference'].tolist()[0])\n",
    "    table['querry'].append(db['querry'].tolist()[0])\n",
    "\n",
    "    # Number of comparisons\n",
    "    table['total_comps'].append(len(db))\n",
    "    table['failed_comps'].append(len(db[db['N_sites'] == 0]))\n",
    "    table['successful_comps'].append(len(db[db['N_sites'] > 0]))\n",
    "    table['considered_bases'].append(db[(db['N_sites'] > 0)]['al_len'].sum())\n",
    "    table['percet_successful'].append((len(db[db['N_sites'] > 0]) / len(db)) * 100)\n",
    "    \n",
    "    # Filter to successful comparisons\n",
    "    db = db[db['N_sites'] > 0]\n",
    "    \n",
    "    # dN/dS stuff\n",
    "    N = db['N_sites'].sum()\n",
    "    table['N_sites'].append(N)\n",
    "    dN = db['N_changed'].sum()\n",
    "    table['N_changed'].append(dN)\n",
    "    S = db['S_sites'].sum()\n",
    "    table['S_sites'].append(S)\n",
    "    dS = db['S_changed'].sum()\n",
    "    table['S_changed'].append(dS)\n",
    "    if (N > 0) & (S > 0) & (dS > 0):\n",
    "        table['dN/dS'].append(((dN/N) / (dS/S)))\n",
    "    else:\n",
    "        table['dN/dS'].append(np.nan)\n",
    "\n",
    "    # Calculate ANI if you must\n",
    "    if fastANI == False:\n",
    "        fastANI = sum([p * l for p, l in zip(db['p_inden'], db['al_len'])]) \\\n",
    "                    / db['al_len'].sum()\n",
    "    table['fast_ani'].append(fastANI)\n",
    "        \n",
    "    # Filter to only ones with a full alignment\n",
    "    if noFilt:\n",
    "        hdb = db[(db['al_len'] > 500)]\n",
    "    else:\n",
    "        hdb = db[(db['al_len'] > 500) & (db['al_len'] < 1000)]\n",
    "                 \n",
    "    ex_iden = _calc_expected(hdb, fastANI)\n",
    "    table['counted_comps'].append(len(hdb))\n",
    "    table['identical_comps'].append(len(hdb[hdb['p_inden'] > 99.99]))\n",
    "    table['expected_identicle'].append(ex_iden)\n",
    "    \n",
    "    try:\n",
    "        table['percent_enriched'].append(((len(hdb[hdb['p_inden'] > 99.99]) - int(ex_iden)) / len(hdb))*100)\n",
    "        hani = sum([p * l for p, l in zip(hdb['p_inden'], hdb['al_len'])]) \\\n",
    "                / hdb['al_len'].sum()\n",
    "        table['filtered_ani'].append(hani)\n",
    "        table['filtered_af'].append(db['al_len'].sum() / db['qry_len'].sum())\n",
    "    except:\n",
    "        table['percent_enriched'].append(0)\n",
    "        if not noFilt:\n",
    "            table['filtered_ani'].append(0)\n",
    "            table['filtered_af'].append(0)   \n",
    "\n",
    "    Ndb = pd.DataFrame(table)\n",
    "    return Ndb\n",
    "\n",
    "def _calc_expected(db, ani):\n",
    "    expected = 0\n",
    "    prob_diff = (100-ani) / 100\n",
    "    prob_same = ani / 100\n",
    "    for i, row in db.iterrows():\n",
    "        # probability of this row being 0\n",
    "        #p = prob_diff * int(row['al_len'])\n",
    "        p = prob_same ** int(row['al_len'])\n",
    "        if p > 1:\n",
    "            expected += 1\n",
    "        else:\n",
    "            expected += p\n",
    "    return expected\n",
    "\n",
    "def parse_to_genomeLevel_HGT(Ddb, noFilt=False):\n",
    "    Tdbs = []\n",
    "    for query, qdb in tqdm(Ddb.groupby('querry')):\n",
    "        for ref, db in qdb.groupby('reference'):\n",
    "            # get the FastANI\n",
    "            try:\n",
    "                fastANI = _get_ani(FAdb, db['reference'].tolist()[0], db['querry'].tolist()[0])\n",
    "            except:\n",
    "                fastANI = 0\n",
    "            \n",
    "            tdb = summarize_dnds_HGT(db, noFilt=noFilt, fastANI=fastANI)\n",
    "            Tdbs.append(tdb)\n",
    "    return pd.concat(Tdbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_dnds(wd_loc):\n",
    "    print(wd_loc)\n",
    "    \n",
    "    #baseloc = wd_loc + 'data/dNdS_data/genomeWide_dNdS_info.csv'\n",
    "    baseloc = wd_loc + 'data/dNdS_data/detailed_dNdS_info.csv'\n",
    "    if os.path.exists(baseloc):\n",
    "        Tdb = pd.read_csv(baseloc)\n",
    "    else:\n",
    "        print(\"ERROR\")\n",
    "        #Tdb = pd.concat([pd.read_csv(t) for t in glob.glob(wd_loc + 'data/dNdS_data/detailed*_chunk_*.csv')])\n",
    "        \n",
    "    return parse_to_genomeLevel_HGT(Tdb, noFilt=True)\n",
    "\n",
    "# Tdb = pd.DataFrame()\n",
    "# for floc in ['/data1/bio_db/refseq/analysis/MAGlists_2/goANI_oceanList/',\n",
    "#              '/data1/bio_db/refseq/analysis/MAGlists_2/goANI_soilList/',\n",
    "#              '/data1/bio_db/refseq/analysis/MAGlists_2/goANI_refseqTheta/',\n",
    "#              '/data1/bio_db/refseq/analysis/MAGlists_2/goANI_infantList//']:\n",
    "#     tdb = load_dnds(floc)\n",
    "#     tdb['method'] = floc.split('/')[6].split('_')[1]\n",
    "#     Tdb = Tdb.append(tdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1 has ocean and soil!\n",
    "#Tdb.to_pickle('/data1/bio_db/refseq/analysis/dn_ds/data_tables/genomeWide_dNdS_info_v6_pt1.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data1/bio_db/refseq/analysis/MAGlists_2/goANI_infantList/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1952/1952 [12:22:00<00:00,  1.22it/s]   \n"
     ]
    }
   ],
   "source": [
    "Tdb = pd.DataFrame()\n",
    "for floc in ['/data1/bio_db/refseq/analysis/MAGlists_2/goANI_infantList/']:\n",
    "    tdb = load_dnds(floc)\n",
    "    tdb['method'] = floc.split('/')[6].split('_')[1]\n",
    "    Tdb = Tdb.append(tdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2 has infant!\n",
    "#Tdb.to_pickle('/data1/bio_db/refseq/analysis/dn_ds/data_tables/genomeWide_dNdS_info_v6_pt2.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 has RefSeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data1/bio_db/refseq/analysis/MAGlists_2/goANI_refseqTheta/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 1764/4363 [9:31:04<9:02:46, 12.53s/it]   "
     ]
    }
   ],
   "source": [
    "Tdb = pd.DataFrame()\n",
    "for floc in ['/data1/bio_db/refseq/analysis/MAGlists_2/goANI_refseqTheta/']:\n",
    "    tdb = load_dnds(floc)\n",
    "    tdb['method'] = floc.split('/')[6].split('_')[1]\n",
    "    Tdb = Tdb.append(tdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tdb.to_pickle('/data1/bio_db/refseq/analysis/dn_ds/data_tables/genomeWide_dNdS_info_v6_pt3.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
